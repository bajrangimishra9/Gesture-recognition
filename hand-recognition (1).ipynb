{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3608858,"sourceType":"datasetVersion","datasetId":2163304},{"sourceId":9613527,"sourceType":"datasetVersion","datasetId":5866385}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input,Dense,Flatten,BatchNormalization,GlobalAveragePooling2D,Dropout,Conv2D,MaxPool2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genobj=ImageDataGenerator(width_shift_range=0.1,height_shift_range=0.1,horizontal_flip=True,rescale=1/255.0,validation_split=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(Image.open('/kaggle/input/hand-gesture-recognition/Dataset_Binary/Dataset_Binary/eight/img 0.jpg')).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE=20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_obj=genobj.flow_from_directory('/kaggle/input/hand-gesture-recognition/Dataset_Binary/Dataset_Binary',target_size=(300,300),batch_size=BATCH_SIZE,subset='training')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_obj=genobj.flow_from_directory('/kaggle/input/hand-gesture-recognition/Dataset_Binary/Dataset_Binary',target_size=(300,300),batch_size=BATCH_SIZE,subset='validation')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_obj.classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_image():\n    list_image_label=[]\n    for i in range(9):\n        batch=next(train_obj)\n        rndm_no=np.random.randint(0,len(next(train_obj)[0]))\n        list_image_label.append((batch[0][rndm_no],batch[1][rndm_no]))\n    return list_image_label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_img=plot_image()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes=plt.subplots(3,3)\naxes=axes.flatten()\ncount=0\nfor i,j in list_img:\n    \n    plt.figure()\n    label=np.where(j==1)[0]\n    axes[count].imshow(i, cmap='gray')\n    axes[count].set_title(f'Label: {label}')\n    axes[count].axis('off')\n    count+=1\nplt.tight_layout()\ntrain_obj.class_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_single(obj):\n    batch=next(obj)\n    image,label=batch[0][0],batch[1][0]\n    return image,label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img,lbl=draw_single(train_obj)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(img)\nplt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape=img.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps_per_epoch=12000//BATCH_SIZE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(300, 300, 3))\n\n# Add custom head for your specific task (e.g., classification)\nx = GlobalAveragePooling2D()(base_model.output)\nx = Dense(1024, activation='relu')(x)\npredictions = Dense(15, activation='softmax')(x)\n\n# Combine base model and custom head\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r=model.fit(train_obj,batch_size=BATCH_SIZE,steps_per_epoch=steps_per_epoch,epochs=10,validation_data=val_obj)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import save_model,load_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_model(model,'/kaggle/working/modelHG_BIN.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = r.history['loss']\nval_loss = r.history['val_loss']\n\n# Create a plot\nplt.figure(figsize=(10, 5))\nplt.plot(train_loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_epoch_index = r.history['val_loss'].index(min(r.history['val_loss']))\nbest_epoch_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train Loss:\",r.history['loss'][0])\nprint(\"Train Accuracy:\",r.history['accuracy'][0])\nprint(\"Validation Loss:\",r.history['val_loss'][0])\nprint(\"Validation Accuracy:\",r.history['val_accuracy'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrieve the true labels\nlabels = val_obj.classes if hasattr(val_obj, 'classes') else None\n\n# Make predictions on the validation set\ny_pred = model.predict(val_obj)\n\n# If labels attribute is not present in val_obj, an alternative is:\n# Convert predicted probabilities to class predictions\nif labels is None:\n    print(\"Warning: 'val_obj' does not have a 'classes' attribute. Please verify the source of true labels.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay, classification_report\nimport numpy as np\n\n# Display the confusion matrix and classification report\nConfusionMatrixDisplay.from_predictions(labels, y_pred)\nprint(classification_report(labels, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UPGRADED CODE","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport keras\nimport matplotlib.pyplot as plt \nimport os \nimport cv2 \nimport numpy as np\nimport pandas as pd\n\n# Import of keras model and hidden layers for our convolutional network\nfrom keras.layers import Conv2D, Activation, MaxPool2D, Dense, Flatten, Dropout\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CATEGORIES = [\n    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', \n    'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'\n]\n\nIMG_SIZE = 50\n\ndata_path = \"/kaggle/input/indian-sign-language-islrtc-referred/original_images\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for binary images \n# Load image data\nimage_data = []\nfor category in CATEGORIES:\n    class_index = CATEGORIES.index(category)\n    path = os.path.join(data_path, category)  # Assuming categories are directly under data_path\n\n    # Check if path exists to avoid FileNotFoundError\n    if not os.path.isdir(path):\n        print(f\"Directory not found: {path}\")\n        continue\n\n    for img in os.listdir(path):\n        try:\n            img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n            if img_arr is not None:  # Check if the image was read successfully\n                resized_img = cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE))\n                image_data.append([resized_img, class_index])\n        except Exception as e:\n            print(f\"Error reading image {img}: {e}\")\n            pass\n\n# Display first item in image_data\nprint(image_data[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_data = []\nfor dr in os.listdir(data_path):\n    for category in CATEGORIES:\n        class_index = CATEGORIES.index(category)\n        path = os.path.join(data_path, dr, category)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n                image_data.append([cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE)), class_index])\n            except Exception as e:\n                pass\nimage_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize list to store image data and labels\nimage_data = []\n\n# Iterate over each category directly in `original_images`\nfor category in CATEGORIES:\n    class_index = CATEGORIES.index(category)\n    path = os.path.join(data_path, category)  # Construct the path for each category\n    \n    # Check if the directory exists\n    if not os.path.exists(path):\n        print(f\"Directory not found: {path}\")\n        continue  # Skip this category if the folder is missing\n\n    # Load images from the category directory\n    for img in os.listdir(path):\n        try:\n            img_path = os.path.join(path, img)\n            img_arr = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n            \n            # Ensure the image was loaded successfully\n            if img_arr is not None:\n                resized_img = cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE))\n                image_data.append([resized_img, class_index])\n            else:\n                print(f\"Failed to load image: {img_path}\")\n        except Exception as e:\n            print(f\"Error processing image {img} in {path}: {e}\")\n\n# Display the first item in the image_data list to verify\nif image_data:\n    print(image_data[0])\nelse:\n    print(\"No images loaded.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nrandom.shuffle(image_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_data = []\nlabel = []\nfor X, y in image_data:\n    input_data.append(X)\n    label.append(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(1, figsize=(10,10))\nfor i in range(1,10):\n    plt.subplot(3,3,i)\n    plt.imshow(image_data[i][0], cmap='gray')\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(CATEGORIES[label[i]][3:])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_data = np.array(input_data)\nlabel = np.array(label)\ninput_data = input_data/255.0\ninput_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras.utils import to_categorical\nlabel = to_categorical(label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_data.shape = (-1, IMG_SIZE, IMG_SIZE, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(input_data, label,test_size=0.2, random_state=42, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model\nmodel = keras.models.Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(IMG_SIZE, IMG_SIZE, 1)))\nmodel.add(Activation('relu'))\n\nmodel.add(Conv2D(filters=32, kernel_size=(3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(36, activation='softmax'))  # Update to 15 units to match number of classes\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=6, batch_size=32, validation_data=(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\nmodel.save(\"hand_gesture_recognition_model_b.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"hand_gesture_recognition_weights.weights.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot model accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\n\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\n\nindex_loss = np.argmin(val_loss)\nindex_acc = np.argmax(val_acc)\n\nval_lowest = val_loss[index_loss]\nval_highest = val_acc[index_acc]\n\nEpochs = [i+1 for i in range(len(train_acc))]\n\nloss_label = f'Best Epoch = {str(index_loss + 1)}'\nacc_label = f'Best Epoch = {str(index_acc + 1)}'\n\nplt.figure(figsize= (20,8))\nplt.style.use('fivethirtyeight')\n\nplt.subplot(1,2,1)\nplt.plot(Epochs , train_loss , 'r', label = 'Training Loss')\nplt.plot(Epochs , val_loss , 'g' , label = 'Validation Loss')\nplt.scatter(index_loss +1 , val_lowest , s = 150 , c = 'blue' , label = loss_label)\nplt.title('Training vs Validation (loss)')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(Epochs , train_acc , 'r', label= 'Training Accuracy')\nplt.plot(Epochs , val_acc , 'g' , label = 'Validation Accuracy')\nplt.scatter(index_acc + 1 , val_highest , s= 150 , c = 'blue' , label= acc_label)\nplt.title('Training vs Validation (Accuracy)')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint('Test accuracy: {:2.2f}%'.format(test_accuracy*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\n\n# Get model predictions\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\ny_true = np.argmax(y_test, axis=1)          # Convert one-hot encoded labels to class labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate model predictions\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\ny_true = np.argmax(y_test, axis=1)          # Convert one-hot encoded labels to class labels\n\n# Generate and display confusion matrix with enhanced visibility\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(30, 20))  # Increase figure size for better clarity\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CATEGORIES)\ndisp.plot(cmap='Blues', xticks_rotation=45)  # Rotate x-axis labels for better readability\n\n# Adjust text size and color bar for clarity\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\n# Access the AxesImage object directly from the display\nplt.colorbar(disp.im_, fraction=0.046, pad=0.04).set_label('Count', size=12)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport numpy as np\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\ny_true = np.argmax(y_test, axis=1)          # Convert one-hot encoded labels to class labels\n\n# Generate the classification report\nreport = classification_report(y_true, y_pred_classes, target_names=CATEGORIES)\nprint(report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"import cv2\nimport numpy as np\nfrom keras.models import load_model\n\n# Load the saved model\nmodel = load_model(\"hand_gesture_recognition_model.h5\")\n\n# Define constants\nIMG_SIZE = 50\nCATEGORIES = ['down', 'eight', 'five', 'four', 'left', 'nine', 'one', 'right', 'seven', 'six', 'stop', 'three', 'two', 'up', 'zero']\n\ndef prepare_image(filepath):\n    \"\"\"\n    Preprocesses the image at 'filepath' to the format required by the model.\n    \"\"\"\n    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)  # Read image in grayscale\n    if img_array is None:\n        print(f\"Error: Unable to read image at {filepath}\")\n        return None\n\n    resized_img = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # Resize to match model input size\n    normalized_img = resized_img / 255.0  # Normalize pixel values to [0,1]\n    reshaped_img = np.reshape(normalized_img, (1, IMG_SIZE, IMG_SIZE, 1))  # Reshape for model input\n    return reshaped_img\n\ndef predict_gesture(filepath):\n    \"\"\"\n    Loads an image, preprocesses it, and predicts the gesture category.\n    \"\"\"\n    # Prepare the image\n    prepared_img = prepare_image(filepath)\n    if prepared_img is None:\n        return \"Invalid image.\"\n\n    # Predict the class\n    predictions = model.predict(prepared_img)\n    predicted_class = np.argmax(predictions[0])\n    confidence = predictions[0][predicted_class]\n    predicted_label = CATEGORIES[predicted_class]\n\n    # Print the result\n    print(f\"Predicted Gesture: {predicted_label} | Confidence: {confidence:.2f}\")\n    return predicted_label, confidence\n\n# Test the function with a new image path\nimage_path = \"/kaggle/input/indian-sign-language-islrtc-referred/original_images/2/1000.jpg\"  # Update with your image path\npredicted_label, confidence = predict_gesture(image_path)\n\n# Save the image with the prediction text overlay\nif predicted_label:\n    img = cv2.imread(image_path)\n    if img is not None:\n        output_image_path = \"predicted_gesture.jpg\"\n        cv2.putText(img, f\"{predicted_label} ({confidence:.2f})\", (10, 30), \n                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n        cv2.imwrite(output_image_path, img)\n        print(f\"Image with prediction saved at {output_image_path}\")\n    else:\n        print(\"Failed to load the image for overlay.\")\n        \"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}